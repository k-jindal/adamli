{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Questions:\n",
    "1. How are these algorithms sensitive to the Lipschitz constant?\n",
    "2. What is a good way to choose L? and correspondingly alpha?\n",
    "\n",
    "Things to Review:\n",
    "1. Proximal Function and writing that out\n",
    "\n",
    "# Least Squares Solver\n",
    "The least squares optimization problem is as follows:\n",
    "\n",
    "$$J(f(x)) := \\frac{1}{2} \\|Ax-b\\|_2^2$$\n",
    "\n",
    "which just uses the L2-norm, square of the Ax=b systems of equations, and minimizes that error.\n",
    "Gradient of cost function:\n",
    "\n",
    "$$\\nabla{f(x)} = A^T(Ax-b)$$\n",
    "\n",
    "Generally, b are observations and A is some sort of model transforming our unknown parameter space/variables into the observation predictions.\n",
    "\n",
    "# Gradient Descent Algorithm\n",
    "To minimize the cost function, J, just compute the gradient of the cost function and step in that direction. For some $\\alpha_{k}$, generally minimizing step size, the iteration is computed in a for loop as follows.\n",
    "\n",
    "$$x_{k+1} = x_{k} - \\alpha_{k}\\nabla{f(x)}$$\n",
    "\n",
    "# ISTA\n",
    "This is similar to the gradient descent algorithm, but is generally used to solve problems that fall in the class of proximal gradient methods. The new cost function is something as follows:\n",
    "\n",
    "$$J(f(x)) := \\frac{1}{2} \\|Ax-b\\|_2^2 + r(x)$$\n",
    "\n",
    "where r(x) can be defined as any proximal, nonsmooth convex function. Examples, include l1, l2, or elastic-net proximal terms. For, those the new cost function is as follows:\n",
    "\n",
    "$$J(f(x)) := \\frac{1}{2} \\|Ax-b\\|_2^2 + \\lambda_1\\|x\\|_1 + \\lambda_2\\|x\\|_2^2$$ or\n",
    "$$J(f(x)) := \\frac{1}{2} \\|Ax-b\\|_2^2 + (1-\\alpha)\\|x\\|_1 + \\alpha\\|x\\|_2^2$$ \n",
    "with \n",
    "$$\\alpha = \\frac{\\lambda_2}{\\lambda_2+\\lambda_1}$$\n",
    "\n",
    "The derivation of the iterative algorithm comes from first-order optimality conditions - 0 must be in the subdifferential of the cost function. The new cost function gives rise to a subproblem of solving for $x_{k+1}$ in two segments, one for f(x) and one for r(x). So just set 0 = the subdifferential and then solve for $x_{k+1}$\n",
    "\n",
    "$$x_{k+1} = Shrink(x_{k} - \\alpha_{k}\\nabla{f(x)};\\lambda\\alpha_k)$$\n",
    "\n",
    "# FISTA\n",
    "This is ISTA, but with the ideas of Nesterov's Acceleration. It has the same general cost functions as ISTA would with some proximal term. However, with an acceleration/extrapolation term, this algorithm performs with O(1/k^2)\n",
    "\n",
    "$$x_{k} = Shrink(Y_{k} - \\alpha_k\\nabla(f(Y));\\lambda\\alpha_k) \\ \\text{Proximal Step}$$ \n",
    "\n",
    "$$t_{k+1} = \\frac{1+\\sqrt{1+4t_k^2}}{2} \\ \\text{Compute Step Size}$$\n",
    "\n",
    "$$y_{k} = x_{k} + \\frac{t_k-1}{t_{k+1}})(x_k - x_{k-1}) \\ \\text{Extrapolation} $$\n",
    "\n",
    "## Step Size\n",
    "All algorithms can use some sort of step size that is arbitrary depending on the problem. A common choice can be using a constant step size = 1/L, where L is the Lipschitz constant for the gradient of the cost function.\n",
    "\n",
    "1. Least Squares\n",
    "    $$L = \\frac{ \\|\\mathbf{x}^T \\mathbf{x}\\|_{op}}{n}$$\n",
    "2. Logistic\n",
    "    $$L = \\frac{\\underset{i}{\\max}(\\|x_i\\|_2^2)}{4n}$$\n",
    "\n",
    "\n",
    "References:\n",
    "1. Elastic Net: http://web.stanford.edu/~hastie/TALKS/enet_talk.pdf\n",
    "2. Code: http://nbviewer.jupyter.org/github/zermelozf/notebooks/blob/master/First%20order%20optimization.ipynb#full\n",
    "https://github.com/JeanKossaifi/FISTA\n",
    "\n",
    "3. FISTA: http://people.rennes.inria.fr/Cedric.Herzet/Cedric.Herzet/Sparse_Seminar/Entrees/2012/11/12_A_Fast_Iterative_Shrinkage-Thresholding_Algorithmfor_Linear_Inverse_Problems_(A._Beck,_M._Teboulle)_files/Breck_2009.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import scipy.optimize\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Cost Functions and Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "A = matrix of features n_samples X n_features\n",
    "x = data = n_features X 1\n",
    "b = observations n_samples X 1\n",
    "'''\n",
    "# def least_squares(x, features, labels):\n",
    "#     \"\"\"Evaluates the least square function.\"\"\"\n",
    "#     n_samples = features.shape[0]\n",
    "#     x = x.reshape(1, n_features)\n",
    "#     loss_array = 1/2*(features.dot(x.T) - labels) ** 2\n",
    "#     return np.sum(loss_array, axis=0)\n",
    "\n",
    "# def least_squares_grad(x, features, labels):\n",
    "#     \"\"\"Evaluates the gradient of the least square function.\"\"\"\n",
    "#     n_samples = features.shape[0]\n",
    "#     x = x.reshape(1, n_features)  # Added for scipy.optimize compatibility\n",
    "#     grad_array = (features.dot(x.T) - labels) * features\n",
    "#     return np.sum(grad_array, axis=0) / n_samples\n",
    "\n",
    "def least_squares(A, x, b):\n",
    "    \"\"\"Evaluates the least square function.\"\"\"\n",
    "    n_samples, n_features = A.shape\n",
    "    x = x.reshape(n_features, 1)\n",
    "    loss_array = 1/2*(A.dot(x) - b) ** 2\n",
    "    return np.sum(loss_array, axis=0)\n",
    "\n",
    "def least_squares_grad(A, x, b):\n",
    "    \"\"\"Evaluates the gradient of the least square function.\"\"\"\n",
    "    n_samples, n_features = A.shape\n",
    "    x = x.reshape(n_features, 1)  # Added for scipy.optimize compatibility\n",
    "    grad_array = (A.dot(x) - b) * A\n",
    "    return (np.sum(grad_array, axis=0) / n_samples).reshape(n_features, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Proximal Operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input: x = xk - step*grad\n",
    "# x_abs > step*lambda_ = shrinkage operator\n",
    "def prox_l1(x, step, lambda_):\n",
    "    \"\"\" Proximal operator of the l1 norm.\"\"\"\n",
    "    x_abs = np.abs(x) # get the absolute value\n",
    "    shrink_op = step*lambda_ # alpha_k * lambda_\n",
    "    return np.sign(x) * (x_abs - shrink_op) * (x_abs > shrink_op)\n",
    "\n",
    "def prox_l2(x, l=1.):\n",
    "    \"\"\" Proximal operator of the l2 norm.\"\"\"\n",
    "    return 1. / (1 + l) * x\n",
    "\n",
    "def prox_enet(x, l_l1, l_l2, t=1.):\n",
    "    \"\"\"Proximal operator for the elastic net at x\"\"\"\n",
    "    x_abs = np.abs(x)\n",
    "    prox_l1 = np.sign(x) * (x_abs - l_l1) * (x_abs > l_l1)\n",
    "    return prox_l1 + 1./ (1. + l_l2) * x_abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inspector(loss_fun, x_real, verbose=False):\n",
    "    \"\"\"A closure called to update metrics after each iteration.\"\"\"\n",
    "    objectives = []\n",
    "    errors = []\n",
    "    it = [0]  # This is a hack to be able to modify 'it' inside the closure.\n",
    "    def inspector_cl(xk):\n",
    "        obj = loss_fun(xk)\n",
    "        err = norm(xk - x_real) / norm(x_real)\n",
    "        objectives.append(obj)\n",
    "        errors.append(err)\n",
    "        if verbose == True:\n",
    "            if it[0] == 0:\n",
    "                print ' | '.join([name.center(8) for name in [\"it\", \"obj\", \"err\"]])\n",
    "            if it[0] % (n_iter / 5) == 0:\n",
    "                print ' | '.join([(\"%d\" % it[0]).rjust(8), (\"%.2e\" % obj).rjust(8), (\"%.2e\" % err).rjust(8)])\n",
    "            it[0] += 1\n",
    "    inspector_cl.obj = objectives\n",
    "    inspector_cl.err = errors\n",
    "    return inspector_cl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def grad_descent(x_init, grad, n_iter=100, step=1., tol=None, callback=None):\n",
    "    x = x_init.copy() # initialize x\n",
    "    \n",
    "    # perform iterations\n",
    "    for _ in range(n_iter):\n",
    "        x -= step * grad(x)\n",
    "        \n",
    "        # update metrics in a function\n",
    "        if callback is not None:\n",
    "            callback(x)\n",
    "            \n",
    "        # if gradient has reached a low point\n",
    "        if tol != None and grad(x) <= tol:\n",
    "            break\n",
    "            \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ista(x_init, grad, prox, n_iter, step=1., cost=least_squares, tol=None, callback=None):\n",
    "    x = x_init.copy() # initialize x\n",
    "    \n",
    "    # perform iterations\n",
    "    for _ in range(n_iter):\n",
    "        x = prox(x - step * grad(x))\n",
    "        \n",
    "        # update metrics in a function\n",
    "        if callback is not None:\n",
    "            callback(x)\n",
    "            \n",
    "        # if gradient has reached a low point\n",
    "        if tol != None and cost(x) <= tol:\n",
    "            break\n",
    "            \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fista(x_init, grad, prox, lambda_, n_iter, step=1., cost=least_squares, tol=None, callback=None):\n",
    "    x = x_init.copy() # initialize x\n",
    "    y = x_init.copy()\n",
    "    t = 1.\n",
    "    \n",
    "    # perform iterations\n",
    "    for _ in range(n_iter):\n",
    "        x_new = prox(y - step * grad(y), step)\n",
    "        t_new = (1. + (1. + 4. * t**2)**.5) / 2\n",
    "        y = x_new + (t - 1) / t_new * (x_new - x)\n",
    "        t = t_new\n",
    "        x = x_new\n",
    "        \n",
    "        # update metrics in a function\n",
    "        if callback is not None:\n",
    "            callback(x)\n",
    "            \n",
    "        # if gradient has reached a low point\n",
    "        if tol != None and cost(x) <= tol:\n",
    "            break\n",
    "            \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cond = 18.54\n",
      "features:  (2000, 50)\n",
      "real weights:  (50, 1)\n",
      "b:  (2000, 1)\n",
      "(50,)\n",
      "prox (50, 1)\n",
      "grad (50, 1)\n",
      "x init:  (50, 1)\n",
      "n_iter: 30\n",
      "step size: 1.12\n",
      "(50, 1)\n",
      "   it    |   obj    |   err   \n",
      "       0 | 0.00e+00 | 8.18e-01\n",
      "       6 | 0.00e+00 | 3.36e-01\n",
      "      12 | 0.00e+00 | 1.96e-01\n",
      "      18 | 0.00e+00 | 1.31e-01\n",
      "      24 | 0.00e+00 | 9.73e-02\n"
     ]
    }
   ],
   "source": [
    "# Generate a fake dataset\n",
    "n_samples = 2000\n",
    "n_features = 50\n",
    "\n",
    "idx = np.arange(n_features).reshape(1, n_features)\n",
    "params = 2 * (-1) ** (idx - 1) * .9**idx\n",
    "params[0, 20:50] = 0\n",
    "params = params.T\n",
    "diag = np.random.rand(n_features)\n",
    "A = np.random.multivariate_normal(np.zeros(n_features), np.diag(diag), n_samples)\n",
    "\n",
    "residuals = np.random.randn(n_samples, 1)\n",
    "b = A.dot(params) + residuals\n",
    "\n",
    "# Show the condition number of the gram matrix\n",
    "print \"cond = %.2f\" % (diag.max() / diag.min())\n",
    "print 'features: ', A.shape\n",
    "print 'real weights: ', params.shape\n",
    "print 'b: ', b.shape\n",
    "\n",
    "# Initialize stuff\n",
    "x_init = 1 - 2 * np.random.rand(n_features, 1)\n",
    "n_iter = 30\n",
    "l_l1 = 0.0\n",
    "l_l2 = 0.1\n",
    "\n",
    "test = (A.dot(x_init) - b) * A\n",
    "test = np.sum(test, axis=0)\n",
    "print test.shape\n",
    "\n",
    "# f and gradient\n",
    "f = lambda x: least_squares(A,x, b)\n",
    "grad_f = lambda x: least_squares_grad(A, x, b)\n",
    "step = norm(A.T.dot(A) / n_samples, 2)\n",
    "\n",
    "# g, F and prox.\n",
    "g = lambda x: l_l1 * np.abs(x).sum()\n",
    "F = lambda x: f(x) + g(x)\n",
    "prox_g = lambda x: prox_l1(x, step, l_l1)\n",
    "\n",
    "print 'prox',prox_g(x_init).shape\n",
    "print 'grad',grad_f(x_init).shape\n",
    "\n",
    "print 'x init: ', x_init.shape\n",
    "print \"n_iter: %d\" % n_iter\n",
    "print \"step size: %.2f\" % step\n",
    "# Gradient descent\n",
    "# Gradient descent\n",
    "grad_gd = lambda x: grad_f(x) + l_l1 * np.abs(x)\n",
    "print grad_gd(x_init).shape\n",
    "\n",
    "gd_inspector = inspector(loss_fun=F, x_real=params, verbose=True)\n",
    "x_gd = grad_descent(x_init, grad=grad_gd, n_iter=n_iter, step=step, callback=gd_inspector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
